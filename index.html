<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">

<!-- Search Engine Description -->
<meta name="description" content="A study of visual descriptor quality using CLIP Similarity and Global Alignment metrics. Evaluates descriptor alignment with CLIP’s training distribution.">

<!-- Open Graph for social media (Facebook, LinkedIn, etc.) -->
<meta property="og:title" content="Beyond Accuracy: Evaluating Visual Descriptors with CLIP Similarity" />
<meta property="og:description" content="We introduce new metrics—Global Alignment and CLIP Similarity—to evaluate the quality of text-based visual descriptors in VLMs." />
<meta property="og:url" content="https://ethan-y-lin.github.io/beyond-accuracy-project-page/" />
<meta property="og:image" content="static/images/fig_concept_main.png" />
<meta property="og:image:width" content="1200" />
<meta property="og:image:height" content="630" />

<!-- Twitter Card Meta -->
<meta name="twitter:title" content="Evaluating Visual Descriptors in CLIP" />
<meta name="twitter:description" content="New metrics for analyzing visual concept quality using vision-language models like CLIP." />
<meta name="twitter:image" content="static/images/fig_concept_main.png" />
<meta name="twitter:card" content="summary_large_image" />

<!-- Keywords for search indexing -->
<meta name="keywords" content="CLIP, vision-language models, visual descriptors, zero-shot learning, ESCHER, representation alignment, CLIP Similarity" />

<!-- Viewport settings for responsiveness -->
<meta name="viewport" content="width=device-width, initial-scale=1">



  <title>Beyond Accuracy: Metrics that Uncover What Makes a 'Good' Visual Descriptor</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Beyond Accuracy: Metrics that Uncover What Makes a 'Good' Visual Descriptor</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors with superscript affiliations -->
              <span class="author-block">
                <a href="https://ethan-y-lin.github.io/" target="_blank">Ethan Lin</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://linxi-zhao.github.io/" target="_blank">Linxi Zhao</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="https://atharvas.net/" target="_blank">Atharva Sehgal</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://jenjsun.com/" target="_blank">Jennifer J. Sun</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <!-- Affiliations -->
              <span class="author-block"><sup>1</sup> Cornell University</span>,
              <span class="author-block"><sup>2</sup> University of Texas at Austin</span><br>
              <span class="author-block">VisCon @ CVPR 2025 <b>[Best Poster Award]</b></span>
            </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openreview.net/pdf/d4e4323197ac1c3472b6f953f0550ef72842938b.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ethan-y-lin/beyond_accuracy" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        Your video here -->
        <!-- <source src="static/pdfs/fig_concept_main.pdf"
        type="image/pdf">
      </video> -->
      <img 
        src="static/images/fig_concept_main.png" 
        alt="Concept figure"
        style="
          display: block;
          margin: 0 auto 1rem;
          max-width: 80%;
          height: auto;
        "
      >
      <h2 class="subtitle has-text-centered">
        Achieving high accuracy does not guarantee that a set of descriptors is "good". 
        Other factors like interpretability may suffer. <b>Global Alignment</b> and 
        <b>CLIP Similarity</b> can serve as new metrics for evaluating and 
        understanding different sets of visual descriptors. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-based visual descriptors—ranging from simple class names to more descriptive phrases—are widely used in visual concept discovery and 
            image classification with vision-language models (VLMs). Their effectiveness, however, depends on a complex interplay of factors, including semantic clarity, 
            presence in the VLM's pre-training data, and how well the descriptors serve as a meaningful representation space. In this work, 
            we systematically analyze descriptor quality along two key dimensions: (1) representational capacity, and (2) relationship with 
            VLM pre-training data. We evaluate a spectrum of descriptor generation methods, from zero-shot LLM-generated prompts to iteratively refined descriptors. 
            Motivated by ideas from representation alignment and language understanding, we introduce two alignment-based metrics—Global Alignment and 
            CLIP Similarity—that move beyond accuracy. These metrics allow us to shed light on how different descriptor generation strategies interact 
            with foundation model properties, offering insights into ways of studying descriptor effectiveness beyond accuracy evaluations.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h3 class="title">Which descriptor set is best?</h3>
      <ul style="
        max-width: 100%;
        margin: 0 auto;
        padding-left: 0;
        list-style-type: none;
        font-size: 1.2rem;  /* Increase font size (e.g., ~20px) */
        line-height: 1.6;    /* Optional: improves readability */
      ">
        <li><strong>Class Name Prompt</strong>: Uses the standard zero-shot image classification format “An image of a {class name}”.</li>
        <li><strong>CBD Concepts</strong>: LLM-generated descriptors from the original Classification by Description framework.</li>
        <li><strong>ESCHER</strong>: Iteratively refined class-specific descriptors using the ESCHER algorithm.</li>
        <li><strong>DCLIP</strong>: Inspired by WaffleCLIP; combines class names with randomly sampled global descriptors.</li>
        <li><strong>WaffleCLIP</strong>: Appends randomized tokens to general concepts and class names, e.g., “An image of a {concept}: {class name}, which has !32d, #tjli, ^fs0.”</li>
      </ul>
      <!-- Button -->
      <button id="performance-button" onclick="togglePerformance()">Show Performance</button>

      <!-- Image container -->
      <div id="performance-container">
        <img 
          id="performance-img"
          src="static/images/cub_accuracy.png" 
          alt="Concept figure"
        >
      </div>

      <!-- Style -->
      <style>
        #performance-button {
          display: block;
          margin: 1rem auto;
          padding: 0.5rem 1rem;
          font-size: 2.0rem;
          background-color: #3273dc;
          color: white;
          border: none;
          border-radius: 5px;
          cursor: pointer;
          transition: background-color 0.3s ease;
        }

        #performance-button:hover {
          background-color: #275fbd; /* darker blue on hover */
        }

        #performance-container {
          max-height: 0;
          overflow: hidden;
          transition: max-height 0.5s ease-out;
          text-align: center;
        }

        #performance-img {
          max-width: 80%;
          height: auto;
          margin-bottom: 1rem;
        }
      </style>

      <!-- Script -->
      <script>
        let isVisible = false;

        function togglePerformance() {
          const container = document.getElementById('performance-container');
          const button = document.getElementById('performance-button');

          if (isVisible) {
            container.style.maxHeight = '0';
            button.textContent = 'Show Performance';
          } else {
            container.style.maxHeight = container.scrollHeight + 'px';
            button.textContent = 'Hide Performance';
          }

          isVisible = !isVisible;
        }
      </script>
      <p>
        Descriptor quality is typically evaluated based on downstream 
        classification accuracy. While informative, this metric offers limited insight
        into why certain descriptors succeed or fail, and provides
        little guidance for descriptor discovery or refinement. 
        Moreover, higher accuracy does not necessarily
        imply more meaningful descriptors. As shown above, random descriptors generation methods
        can outperform zero-shot LLM-generation methods like the original Classification by Description method or
        even an iterative refinement algorithm like ESCHER. Even though the semantic quality of the random descriptors is much 
        worse, they still have better accuracy. Clearly, there is need for more principled methods to assess 
        descriptor quality beyond accuracy.
      </p>
    </div>
  </div>
</section>
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h3 class="title">Contributions</h3>
      <p>
        In this work, we propose a novel approach to assessing descriptor quality by probing the relationship between
        textual descriptors and the underlying VLM. We design two new metrics:
      </p>

      <div style="margin-top: 1.5rem; ">
        <p style="font-size: 1.3rem; margin-bottom: 0.75rem;">
          1) <b>Global Alignment:</b> Measures the representational capacity of a set of descriptors.
        </p>
        <p style="font-size: 1.3rem; margin-bottom: 0.75rem;">
          2) <b>CLIP Similarity:</b> Measures how well a set of descriptors aligns with the VLM's pre-training data.
        </p>
      </div>
    </div>
  </div>
</section>
<section class="hero">
    <div class="hero-body">
      <div class="container" style="display: flex; align-items: top; gap: 2rem; flex-wrap: wrap;">
        
        <!-- Text Content -->
        <div style="flex: 1; min-width: 300px;">
          <h3 class="title">Global Alignment</h3>
          <p>
            Classification by description using VLMs can be viewed
            as a semantic projection, where images are mapped
            to similarity scores over a set of textual descriptors. This
            forms a new <b>representation space</b>, with each dimension corresponding to a specific concept. 
            Conceptually, this resembles principal component analysis, where we aim to find a set 
            of bases in the form of natural language.
          </p>
          <p>
            <b>Global Alignment</b> evaluates the quality of this descriptor space by using metrics from previous work in <b>representational alignment</b>. 
            We measure the Mutual-KNN alignment between the visual descriptor space to a strong reference image embedding space. A high alignment value indicates that the 
            descriptors capture meaningful visual structure which likely corresponds to high semantic quality.
          </p>
        </div>

        <!-- Image -->
        <div style="flex: 1; min-width: 300px; text-align: center;">
          <img src="static/images/semantic_proj_fig.png" alt="Semantic Projection" style="max-width: 80%; height: auto;">
        </div>

      </div>
      <div class="container" style="text-align: center;">
        <h4 class="title is-4">Global Alignment Method</h4>

        <figure style="margin: 0 auto; max-width: 80%;">
          <img 
            src="static/images/Global_Alignment_Method.png" 
            alt="Global Alignment Method" 
            style="width: 100%; height: auto; display: block; margin: 0 auto;"
          >
          <figcaption style="font-size: 1rem; margin-top: 0.5rem; color: #555;">
            Our alignment metric compares neighborhood structure between the projections of a dataset into descriptor and 
            image embedding spaces.
          </figcaption>
        </figure>
      </div>
    </div>
</section>
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h4 class="title">Global Alignment Results</h4>
      We compute the alignment and accuracy across three-different datasets that are commonly used in Classification by Description frameworks. While there is no
      clear trend in accuracy across the different sets of descriptors, alignment shows a clear trend in representation quality across the different types of descriptor sets.
      <div style="display: flex; justify-content: center; gap: 1.5rem; flex-wrap: wrap;">
        <img src="static/images/nabirds_alignment_vs_accuracy.png" alt="NABirds Alignment vs. Accuracy" style="max-width: 30%; height: auto;">
        <img src="static/images/cifar100_alignment_vs_accuracy.png" alt="CIFAR100 Alignment vs. Accuracy" style="max-width: 30%; height: auto;">
        <img src="static/images/cub_alignment_vs_accuracy.png" alt="CUB Alignment vs. Accuracy" style="max-width: 30%; height: auto;">
      </div>
    </div>
  </div>
</section>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<section class="hero">
  <div class="hero-body">
    <div class="container">
      <h2 class="title"> CLIP Similarity </h2>
      <figure style="margin: 0 auto; max-width: 80%;">
        <img 
          src="static/images/CLIP_SIM.png" 
          alt="CLIP Similarity Metric" 
          style="width: 100%; height: auto; display: block; margin: 0 auto;"
        >
        <figcaption style="font-size: 1rem; margin-top: 0.5rem; color: #555;">
          <b>CLIP Similarity</b> assesses how well descriptors align with visual content by retrieving 
          related image-text pairs from CLIP’s pre-training dataset and averaging their similarity scores.
        </figcaption>
      </figure>
    </br>
      <p>
        Due to the imbalanced nature of CLIP’s training data, semantically similar text descriptors may not yield similar downstream performance. 
        To analyze this, we propose <strong>CLIP Similarity</strong>, a metric that measures how well descriptor candidates align with CLIP's pre-training data.
      </p>
    </br>
      <figure style="margin: 0 auto; max-width: 80%;">
        <img 
          src="static/images/Clip_Similarity_Method.png" 
          alt="CLIP Similarity Method" 
          style="width: 100%; height: auto; display: block; margin: 0 auto;"
        >
        <figcaption style="font-size: 1rem; margin-top: 0.5rem; color: #555;">
          Diagram of how the CLIP Similarity metric is computed given a set of descriptors and CLIP's pre-training data. 
        </figcaption>
      </figure>
      </br>
      <p>
        We define two statistics for each descriptor \( d \), based on its top-\( k \) nearest captions \( \{c_1, \dots, c_k\} \) 
        retrieved by cosine similarity in CLIP’s text embedding space. Let \( \text{sim}(a, b) \) denote cosine similarity between two L2-normalized embeddings.
      </p>

      <ul style="margin-top: 1rem;">
        <li style="margin-bottom: 1rem;">
          <strong>Frequency:</strong> number of captions above a similarity threshold \( \tau \):
          <div>
            \[
            \mathcal{I}_d = \left\{ i \in [k] \mid \text{sim}(d, c_i) > \tau \right\}, \quad
            \text{Freq}(d) = |\mathcal{I}_d|
            \]
          </div>
        </li>

        <li>
          <strong>Similarity:</strong> average similarity between each matched caption \( c_i \) and its paired image \( I_i \):
          <div>
            \[
            \text{Sim}(d) = \frac{1}{|\mathcal{I}_d|} \sum_{i \in \mathcal{I}_d} \text{sim}(c_i, I_i)
            \]
          </div>
        </li>
      </ul>

      <p>
        These two metrics capture how frequently a text descriptor appears in CLIP’s training 
        set and how well it aligns with visual content. This provides a proxy for how 
        compatible a descriptor is with CLIP’s vision-language representation space.
        High-quality descriptors should not only distinguish between image classes but 
        also align with the inductive biases learned by VLMs. Such descriptors are more likely 
        to yield better downstream performance.
      </p>

      <p><strong>CLIP Similarity Score:</strong> Average similarity across all descriptors:
        <div>
          \[
          \text{CLIP Similarity} = \frac{1}{|\mathcal{D}|} \sum_{d \in \mathcal{D}} \text{Sim}(d)
          \]
        </div>
      </p>
    </div>
  </div>
</section>
<section class="hero is-light">
  <div class="hero-body">
    <div class="container">
      <h4 class="title">CLIP Similarity Results</h4>
      <p>
        To build intuition about CLIP’s priors, we first analyze
        the relationship between descriptor frequency and CLIP
        similarity. As shown below, we observe a <b>consistent
        negative correlation between frequency and image-text similarity</b>—a somewhat counterintuitive trend. 
        One possible explanation is that frequent descriptors tend to be coarser
        and more ambiguous (e.g., “animal”), making it harder for CLIP to associate them with a consistent visual pattern. In
        contrast, rarer descriptors often refer to more specific, visually grounded concepts that CLIP can align with more
        reliably.
      </p>
    </div>
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item">
          <h2 class="title is-4"> <strong>Frequency</strong> vs. <strong>CLIP Similarity</strong> on NABirds</h2>
          <!-- Your image here -->
          <img src="static/images/frequency_vs_clip_sim_nabirds.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Distribution of descriptors by frequency and corresponding CLIP Similarity on the <strong>NABirds</strong> dataset.
          </h2>
        </div>
        <div class="item">
          <h2 class="title is-4"> <strong>Frequency</strong> vs. <strong>CLIP Similarity</strong> on CIFAR100</h2>
          <!-- Your image here -->
          <img src="static/images/frequency_vs_clip_sim_cifar100.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
            Distribution of descriptors by frequency and corresponding CLIP Similarity on the <strong>CIFAR100</strong> dataset.
          </h2>
        </div>
        <div class="item">
          <h2 class="title is-4"> <strong>Frequency</strong> vs. <strong>CLIP Similarity</strong> on CUB</h2>
          <!-- Your image here -->
          <img src="static/images/frequency_vs_clip_sim_cub.png" alt="MY ALT TEXT"/>
          <h2 class="subtitle has-text-centered">
          Distribution of descriptors by frequency and corresponding CLIP Similarity on the <strong>CUB</strong> dataset.
          </h2>
        </div>
      </div>
    </div>
    </br>
    <div class="container">
      <h2 class="title is-4">
        Does an iterative algorithm like ESCHER converge towards descriptors that align with CLIP's vision-language priors?
      </h2>
      <div style="display: flex; justify-content: center; gap: 1.5rem; flex-wrap: wrap; margin-top: 2rem;">
        
        <!-- CIFAR-100 -->
        <div style="text-align: center; max-width: 30%;">
          <h4 class="subtitle is-5"><strong>CIFAR100</strong></h4>
          <img src="static/images/sim_iteration_cifar100.png" alt="CIFAR100 CLIP Similarity vs. Accuracy" style="width: 100%; height: auto;">
        </div>

        <!-- CUB -->
        <div style="text-align: center; max-width: 30%;">
          <h4 class="subtitle is-5"><strong>CUB</strong></h4>
          <img src="static/images/sim_iteration_cub.png" alt="CUB CLIP Similarity vs. Accuracy" style="width: 100%; height: auto;">
        </div>

        <!-- NABirds -->
        <div style="text-align: center; max-width: 30%;">
          <h4 class="subtitle is-5"><strong>NABirds</strong></h4>
          <img src="static/images/sim_iteration_nabirds.png" alt="NABirds CLIP Similarity vs. Accuracy" style="width: 100%; height: auto;">
        </div>

      </div>

      <p style="margin-top: 2rem;">
        We use CLIP Similarity to evaluate whether or not descriptor refinement moves toward CLIP’s training distribution.
        The figure above shows how descriptor quality evolves over iterations of ESCHER. Across all datasets, 
        CLIP Similarity generally increases across ESCHER iterations even when accuracy does not. <b>This suggests that descriptors 
        become increasingly fine-grained and better aligned with CLIP's representation space.</b>
      </p>
    </div>

  </div>

</section>
<!-- Paper poster -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/beyond_accuracy_poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
